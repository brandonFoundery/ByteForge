#!/usr/bin/env python3
"""
Frontend Test Generator for FY.WB.Midway
Generates comprehensive testRigor test plans and implementations for frontend testing.
"""

import os
import json
import asyncio
import logging
import traceback
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn

console = Console()

# Set up logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('frontend_test_generator.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


@dataclass
class TestPlanResult:
    """Result of test plan generation"""
    success: bool
    test_plan_path: Optional[str] = None
    pages_analyzed: int = 0
    test_tasks_created: int = 0
    execution_time: float = 0.0
    error_summary: Optional[str] = None


@dataclass
class TestGenerationResult:
    """Result of testRigor test generation"""
    success: bool
    tests_directory: Optional[str] = None
    tests_generated: int = 0
    execution_time: float = 0.0
    error_summary: Optional[str] = None


@dataclass
class CompleteWorkflowResult:
    """Result of complete testing workflow"""
    success: bool
    test_plan_path: Optional[str] = None
    tests_directory: Optional[str] = None
    total_execution_time: float = 0.0
    pages_analyzed: int = 0
    tests_generated: int = 0
    error_summary: Optional[str] = None


class FrontendTestGenerator:
    """Generates comprehensive frontend test plans and testRigor implementations"""
    
    def __init__(self, project_name: str, base_path: Path, model_provider: str = "anthropic"):
        self.project_name = project_name
        self.base_path = base_path
        self.model_provider = model_provider
        self.frontend_path = base_path / "FrontEnd"
        self.testing_output_path = base_path / "generated_documents" / "testing"
        self.claude_prompts_path = base_path / "Development_Prompts" / "claude_code_prompts"
        
        # Ensure output directories exist
        self.testing_output_path.mkdir(parents=True, exist_ok=True)
        self.claude_prompts_path.mkdir(parents=True, exist_ok=True)
    
    async def generate_test_plan(self) -> TestPlanResult:
        """Generate comprehensive frontend test plan using Claude Code"""
        start_time = datetime.now()
        logger.info("Starting test plan generation")

        try:
            console.print("[cyan]Step 1: Analyzing frontend codebase structure...[/cyan]")
            logger.info("Analyzing frontend codebase structure")

            # Analyze frontend structure
            pages_info = await self._analyze_frontend_structure()
            logger.info(f"Found {len(pages_info)} pages to analyze")

            console.print(f"[green]Found {len(pages_info)} pages to analyze[/green]")

            # Generate Claude Code prompt for test plan generation
            console.print("[cyan]Step 2: Generating Claude Code prompt for test plan analysis...[/cyan]")
            logger.info("Generating Claude Code prompt for test plan analysis")

            test_plan_prompt = await self._create_test_plan_prompt(pages_info)
            logger.info(f"Generated test plan prompt with {len(test_plan_prompt)} characters")

            # Save the prompt for Claude Code
            prompt_file = self.claude_prompts_path / "frontend_test_plan_prompt.md"
            logger.info(f"Saving prompt to: {prompt_file}")

            with open(prompt_file, 'w', encoding='utf-8') as f:
                f.write(test_plan_prompt)

            console.print(f"[green]✅ Claude Code prompt saved to: {prompt_file}[/green]")

            # Create instruction file for Claude Code execution
            instruction_file = self.claude_prompts_path / "test_plan_instructions.md"
            logger.info(f"Creating instruction file: {instruction_file}")

            await self._create_test_plan_instructions(instruction_file)

            console.print(f"[green]✅ Claude Code instructions saved to: {instruction_file}[/green]")

            # Launch Claude Code terminal for test plan generation
            console.print("[cyan]Step 3: Launching Claude Code terminal for test plan generation...[/cyan]")
            logger.info("Launching Claude Code terminal for test plan generation")

            await self._launch_claude_code_terminal(
                "Frontend Test Plan Generation",
                str(instruction_file),
                str(self.frontend_path)
            )

            # The actual test plan will be generated by Claude Code
            test_plan_path = self.testing_output_path / "frontend_test_plan.md"

            execution_time = (datetime.now() - start_time).total_seconds()
            logger.info(f"Test plan generation completed in {execution_time:.2f} seconds")

            return TestPlanResult(
                success=True,
                test_plan_path=str(test_plan_path),
                pages_analyzed=len(pages_info),
                test_tasks_created=0,  # Will be filled by Claude Code
                execution_time=execution_time
            )

        except Exception as e:
            execution_time = (datetime.now() - start_time).total_seconds()
            logger.error(f"Test plan generation failed: {str(e)}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            console.print(f"[red]❌ Error in test plan generation: {str(e)}[/red]")

            return TestPlanResult(
                success=False,
                execution_time=execution_time,
                error_summary=str(e)
            )
    
    async def generate_testrigor_tests(self) -> TestGenerationResult:
        """Generate testRigor tests based on the test plan"""
        start_time = datetime.now()
        
        try:
            console.print("[cyan]Step 1: Reading test plan...[/cyan]")
            
            test_plan_path = self.testing_output_path / "frontend_test_plan.md"
            if not test_plan_path.exists():
                raise FileNotFoundError(f"Test plan not found: {test_plan_path}")
            
            # Generate Claude Code prompt for testRigor test generation
            console.print("[cyan]Step 2: Generating Claude Code prompt for testRigor test generation...[/cyan]")
            
            testrigor_prompt = await self._create_testrigor_generation_prompt()
            
            # Save the prompt for Claude Code
            prompt_file = self.claude_prompts_path / "testrigor_generation_prompt.md"
            with open(prompt_file, 'w', encoding='utf-8') as f:
                f.write(testrigor_prompt)
            
            console.print(f"[green]✅ Claude Code prompt saved to: {prompt_file}[/green]")
            
            # Create instruction file for Claude Code execution
            instruction_file = self.claude_prompts_path / "testrigor_generation_instructions.md"
            await self._create_testrigor_generation_instructions(instruction_file)
            
            console.print(f"[green]✅ Claude Code instructions saved to: {instruction_file}[/green]")
            
            # Launch Claude Code terminal for testRigor generation
            console.print("[cyan]Step 3: Launching Claude Code terminal for testRigor test generation...[/cyan]")

            await self._launch_claude_code_terminal(
                "testRigor Test Generation",
                str(instruction_file),
                str(self.testing_output_path)
            )

            tests_directory = self.testing_output_path / "testrigor_tests"
            execution_time = (datetime.now() - start_time).total_seconds()

            return TestGenerationResult(
                success=True,
                tests_directory=str(tests_directory),
                tests_generated=0,  # Will be filled by Claude Code
                execution_time=execution_time
            )
            
        except Exception as e:
            execution_time = (datetime.now() - start_time).total_seconds()
            return TestGenerationResult(
                success=False,
                execution_time=execution_time,
                error_summary=str(e)
            )
    
    async def run_complete_workflow(self) -> CompleteWorkflowResult:
        """Run the complete testing workflow"""
        start_time = datetime.now()

        try:
            console.print("[bold cyan]Starting complete frontend testing workflow...[/bold cyan]")

            # Step 1: Generate test plan
            console.print("\n[yellow]Phase 1: Generating test plan...[/yellow]")
            test_plan_result = await self.generate_test_plan()

            if not test_plan_result.success:
                raise Exception(f"Test plan generation failed: {test_plan_result.error_summary}")

            # Wait for user to complete Phase 1
            console.print("\n[bold yellow]⚠️  Phase 1 Complete - Claude Code Terminal Launched[/bold yellow]")
            console.print("[cyan]Please complete the test plan generation in the Claude Code terminal.[/cyan]")
            console.print("[cyan]The terminal will guide you through analyzing the frontend and creating the test plan.[/cyan]")
            console.print("\n[bold]When you have completed the test plan generation:[/bold]")
            console.print("1. The test plan should be saved to: generated_documents/testing/frontend_test_plan.md")
            console.print("2. Return to this terminal and press Enter to continue to Phase 2")

            input("\n[Press Enter when Phase 1 is complete and test plan is generated...]")

            # Verify test plan was created
            test_plan_path = self.testing_output_path / "frontend_test_plan.md"
            if not test_plan_path.exists():
                console.print(f"\n[red]❌ Test plan not found at: {test_plan_path}[/red]")
                console.print("[yellow]Please ensure the test plan was generated successfully before proceeding.[/yellow]")
                raise Exception("Test plan file not found after Phase 1 completion")

            console.print(f"\n[green]✅ Test plan found: {test_plan_path}[/green]")

            # Step 2: Generate testRigor tests
            console.print("\n[yellow]Phase 2: Generating testRigor tests...[/yellow]")
            test_generation_result = await self.generate_testrigor_tests()

            if not test_generation_result.success:
                raise Exception(f"Test generation failed: {test_generation_result.error_summary}")

            # Wait for user to complete Phase 2
            console.print("\n[bold yellow]⚠️  Phase 2 Complete - Claude Code Terminal Launched[/bold yellow]")
            console.print("[cyan]Please complete the testRigor test generation in the Claude Code terminal.[/cyan]")
            console.print("[cyan]The terminal will guide you through converting the test plan to testRigor tests.[/cyan]")
            console.print("\n[bold]When you have completed the testRigor test generation:[/bold]")
            console.print("1. The tests should be saved to: generated_documents/testing/testrigor_tests/")
            console.print("2. Return to this terminal and press Enter to complete the workflow")

            input("\n[Press Enter when Phase 2 is complete and testRigor tests are generated...]")

            total_execution_time = (datetime.now() - start_time).total_seconds()

            return CompleteWorkflowResult(
                success=True,
                test_plan_path=test_plan_result.test_plan_path,
                tests_directory=test_generation_result.tests_directory,
                total_execution_time=total_execution_time,
                pages_analyzed=test_plan_result.pages_analyzed,
                tests_generated=test_generation_result.tests_generated
            )

        except Exception as e:
            total_execution_time = (datetime.now() - start_time).total_seconds()
            return CompleteWorkflowResult(
                success=False,
                total_execution_time=total_execution_time,
                error_summary=str(e)
            )
    
    async def _analyze_frontend_structure(self) -> List[Dict[str, Any]]:
        """Analyze the frontend directory structure to identify pages and components"""
        logger.info("Starting frontend structure analysis")
        pages_info = []

        try:
            # Check if frontend path exists
            logger.info(f"Checking frontend path: {self.frontend_path}")
            if not self.frontend_path.exists():
                logger.error(f"Frontend path does not exist: {self.frontend_path}")
                return pages_info

            # Look for Next.js pages
            pages_dir = self.frontend_path / "src" / "pages"
            logger.info(f"Checking pages directory: {pages_dir}")

            if pages_dir.exists():
                logger.info("Pages directory found, scanning for .tsx files")
                page_files = list(pages_dir.rglob("*.tsx"))
                logger.info(f"Found {len(page_files)} .tsx files in pages directory")

                for page_file in page_files:
                    if not page_file.name.startswith("_"):  # Skip Next.js special files
                        try:
                            route = self._get_route_from_file(page_file, pages_dir)
                            page_info = {
                                "path": str(page_file.relative_to(self.frontend_path)),
                                "name": page_file.stem,
                                "type": "page",
                                "route": route
                            }
                            pages_info.append(page_info)
                            logger.debug(f"Added page: {page_info}")
                        except Exception as e:
                            logger.error(f"Error processing page file {page_file}: {str(e)}")
            else:
                logger.warning(f"Pages directory not found: {pages_dir}")

            # Look for components
            components_dir = self.frontend_path / "src" / "components"
            logger.info(f"Checking components directory: {components_dir}")

            if components_dir.exists():
                logger.info("Components directory found, scanning for .tsx files")
                component_files = list(components_dir.rglob("*.tsx"))
                logger.info(f"Found {len(component_files)} .tsx files in components directory")

                for component_file in component_files:
                    try:
                        component_info = {
                            "path": str(component_file.relative_to(self.frontend_path)),
                            "name": component_file.stem,
                            "type": "component"
                        }
                        pages_info.append(component_info)
                        logger.debug(f"Added component: {component_info}")
                    except Exception as e:
                        logger.error(f"Error processing component file {component_file}: {str(e)}")
            else:
                logger.warning(f"Components directory not found: {components_dir}")

            logger.info(f"Frontend structure analysis completed. Found {len(pages_info)} items total")
            return pages_info

        except Exception as e:
            logger.error(f"Error in frontend structure analysis: {str(e)}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            raise
    
    def _get_route_from_file(self, file_path: Path, pages_dir: Path) -> str:
        """Convert file path to Next.js route"""
        relative_path = file_path.relative_to(pages_dir)
        route = "/" + str(relative_path.with_suffix("")).replace("\\", "/")
        
        # Handle index files
        if route.endswith("/index"):
            route = route[:-6] or "/"
        
        return route

    async def _create_test_plan_prompt(self, pages_info: List[Dict[str, Any]]) -> str:
        """Create Claude Code prompt for test plan generation"""
        logger.info("Creating test plan prompt")

        try:
            pages_list = "\n".join([
                f"- {page['type'].title()}: {page['name']} ({page.get('route', page['path'])})"
                for page in pages_info
            ])
            logger.info(f"Generated pages list with {len(pages_info)} items")

            prompt = f"""# Frontend Test Plan Generation for {self.project_name}

## Objective
You are a senior QA engineer tasked with creating a comprehensive test plan for the {self.project_name} frontend application. Your goal is to analyze each page and component, identify all interactive elements, and create detailed test tasks for testRigor automation.

## Frontend Structure Analysis
The following pages and components have been identified:

{pages_list}

## Your Tasks

### 1. Page Analysis
For each page in the frontend:
1. **Open and examine the page source code**
2. **Identify the page's purpose and functionality**
3. **Document the page's style compliance** (check against style_guide.html if available)
4. **Verify the page loads correctly**

### 2. Interactive Element Analysis
For each page, identify and document:
- **Buttons**: All clickable buttons and their expected actions
- **Forms**: Input fields, dropdowns, checkboxes, radio buttons
- **Navigation**: Menu items, links, breadcrumbs
- **Modals**: Pop-up dialogs and their triggers
- **Data Tables**: Sorting, filtering, pagination
- **Search**: Search boxes and filters
- **File Uploads**: File input controls
- **Dynamic Content**: AJAX-loaded content, real-time updates

### 3. Modal Analysis
For each modal identified:
1. **Document the modal trigger** (button/link that opens it)
2. **List all controls within the modal**
3. **Identify the modal's purpose** (create, edit, delete, view)
4. **Document close/cancel actions**

### 4. Test Task Creation
Create specific test tasks in this format:

```markdown
## Test Tasks for [Page Name]

### Page Load and Style Tests
- [ ] **Load Test**: Navigate to [route] and verify page loads without errors
- [ ] **Style Compliance**: Verify page matches style guide requirements
- [ ] **Responsive Design**: Test page layout on mobile (375px) and desktop (1920px)

### Interactive Element Tests
- [ ] **[Element Name]**: [Specific test action and expected result]
- [ ] **[Form Name]**: Fill form with valid data and submit, verify success
- [ ] **[Button Name]**: Click button and verify [expected action]

### Modal Tests (if applicable)
- [ ] **[Modal Name] - Open**: Click [trigger] and verify modal opens
- [ ] **[Modal Name] - Controls**: Test all controls within modal
- [ ] **[Modal Name] - Close**: Verify modal closes properly
```

## Output Requirements

Create a comprehensive markdown document with:

1. **Executive Summary**: Overview of pages analyzed and test coverage
2. **Page Inventory**: Complete list of all pages with descriptions
3. **Test Tasks by Page**: Detailed test tasks for each page
4. **Modal Test Tasks**: Separate section for modal testing
5. **Cross-Page Tests**: Navigation and workflow tests
6. **Performance Tests**: Page load time and responsiveness tests
7. **Error Handling Tests**: 404, 500, and validation error scenarios

## Quality Standards

- Each test task should be **specific and actionable**
- Include **expected results** for each test
- Cover **both positive and negative test cases**
- Ensure **comprehensive coverage** of all interactive elements
- Follow **testRigor natural language** patterns

## File Output
Save the complete test plan as: `generated_documents/testing/frontend_test_plan.md`

Begin your analysis now by examining each page file and creating the comprehensive test plan.
"""
            logger.info(f"Generated test plan prompt with {len(prompt)} characters")
            return prompt

        except Exception as e:
            logger.error(f"Error creating test plan prompt: {str(e)}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            raise

    async def _create_test_plan_instructions(self, instruction_file: Path) -> None:
        """Create Claude Code instructions for test plan generation"""

        instructions = f"""# Claude Code Instructions: Frontend Test Plan Generation

## Context
You are generating a comprehensive test plan for the {self.project_name} frontend application. This test plan will be used to create testRigor automation tests.

## Directory Context
- **Frontend Source**: `FrontEnd/src/`
- **Pages Directory**: `FrontEnd/src/pages/`
- **Components Directory**: `FrontEnd/src/components/`
- **Style Guide**: `FrontEnd/style_guide.html`
- **Output Directory**: `generated_documents/testing/`

## Commands to Execute

1. **Analyze Frontend Structure**:
   ```bash
   find FrontEnd/src/pages -name "*.tsx" -type f | head -20
   find FrontEnd/src/components -name "*.tsx" -type f | head -10
   ```

2. **Check Frontend Server Status**:
   ```bash
   echo "Frontend should be running on http://localhost:4001"
   curl -s -o /dev/null -w "%{{http_code}}" http://localhost:4001 || echo "Frontend not running"
   ```

3. **Examine Key Pages** (examine at least these critical pages):
   ```bash
   cat FrontEnd/src/pages/index.tsx
   cat FrontEnd/src/pages/login.tsx
   cat FrontEnd/src/pages/dashboard.tsx
   cat FrontEnd/src/pages/admin/index.tsx
   cat FrontEnd/src/pages/customers/index.tsx
   cat FrontEnd/src/pages/loads/index.tsx
   ```

3. **Check Style Guide**:
   ```bash
   cat FrontEnd/style_guide.html | head -50
   ```

4. **Create Output Directory**:
   ```bash
   mkdir -p generated_documents/testing
   ```

5. **Generate Test Plan**:
   Create the comprehensive test plan file following the prompt requirements.

## Success Criteria
- [ ] All pages analyzed and documented
- [ ] Interactive elements identified for each page
- [ ] Modal components documented with triggers
- [ ] Test tasks created in actionable format
- [ ] Output saved to correct location
- [ ] Test plan follows testRigor natural language patterns

## Notes
- Focus on **actionable test tasks** that can be automated
- Include **both UI and functional testing**
- Consider **error scenarios** and edge cases
- Ensure **comprehensive coverage** of user workflows
"""

        with open(instruction_file, 'w', encoding='utf-8') as f:
            f.write(instructions)

    async def _create_testrigor_generation_prompt(self) -> str:
        """Create Claude Code prompt for testRigor test generation"""

        prompt = f"""# testRigor Test Generation for {self.project_name}

## Objective
You are a senior test automation engineer tasked with converting the frontend test plan into executable testRigor test scripts. Your goal is to create comprehensive, maintainable testRigor tests that follow natural language patterns.

## Input
- **Test Plan**: `generated_documents/testing/frontend_test_plan.md`
- **testRigor Prompt Templates**: `Development_Prompts/testRigor_*.md`

## Your Tasks

### 1. Read and Analyze Test Plan
1. **Load the test plan** from the generated file
2. **Parse test tasks** by page and category
3. **Identify test dependencies** and prerequisites
4. **Group related tests** for efficient execution

### 2. Generate testRigor Tests
For each test task in the plan, create testRigor test scripts using natural language:

#### Test Structure:
```
[Test Name]

# Test setup and navigation
open url "http://localhost:4001[route]"
wait until page loads

# Test steps (natural language)
[specific test actions]

# Verification steps
check that page does not contain "Error"
check that page does not contain "404"
save screenshot as "[test-name]-result"
```

### 3. Test Categories to Generate

#### Page Load Tests
- Basic page loading and error checking
- Style compliance verification
- Responsive design testing

#### Interactive Element Tests
- Button click tests with expected outcomes
- Form submission tests with validation
- Navigation tests between pages

#### Modal Tests
- Modal opening and closing
- Modal form interactions
- Modal error handling

#### Workflow Tests
- End-to-end user workflows
- Cross-page navigation scenarios
- Data persistence tests

### 4. testRigor Best Practices
- Use **natural language** descriptions for elements
- Include **wait strategies** for dynamic content
- Add **screenshot capture** at key points
- Implement **error checking** after each action
- Use **variables** for reusable test data

## Output Structure

Create organized test files:

```
generated_documents/testing/testrigor_tests/
├── page_load_tests/
│   ├── homepage_load_test.txt
│   ├── login_page_load_test.txt
│   └── dashboard_load_test.txt
├── interactive_tests/
│   ├── login_form_test.txt
│   ├── customer_creation_test.txt
│   └── load_management_test.txt
├── modal_tests/
│   ├── customer_modal_test.txt
│   └── load_modal_test.txt
├── workflow_tests/
│   ├── complete_customer_workflow.txt
│   └── load_creation_workflow.txt
└── master_test_suite.txt
```

## Quality Standards
- Each test should be **executable in testRigor**
- Use **clear, descriptive test names**
- Include **comprehensive error checking**
- Follow **testRigor natural language syntax**
- Ensure **test independence** (each test can run standalone)

Begin by reading the test plan and generating the testRigor test scripts.
"""
        return prompt

    async def _create_testrigor_generation_instructions(self, instruction_file: Path) -> None:
        """Create Claude Code instructions for testRigor test generation"""

        instructions = f"""# Claude Code Instructions: testRigor Test Generation

## Context
You are converting the frontend test plan into executable testRigor test scripts for the {self.project_name} application.

## Input Files
- **Test Plan**: `generated_documents/testing/frontend_test_plan.md`
- **testRigor Templates**: `Development_Prompts/testRigor_*.md`

## Commands to Execute

1. **Read Test Plan**:
   ```bash
   cat generated_documents/testing/frontend_test_plan.md
   ```

2. **Review testRigor Templates**:
   ```bash
   ls Development_Prompts/testRigor_*.md
   cat Development_Prompts/testRigor_FY_WB_Midway_Implementation.md
   ```

3. **Create Test Directory Structure**:
   ```bash
   mkdir -p generated_documents/testing/testrigor_tests/page_load_tests
   mkdir -p generated_documents/testing/testrigor_tests/interactive_tests
   mkdir -p generated_documents/testing/testrigor_tests/modal_tests
   mkdir -p generated_documents/testing/testrigor_tests/workflow_tests
   ```

4. **Generate Test Files**:
   Create individual testRigor test files for each category following the prompt requirements.

## testRigor Syntax Reference
- `open url "http://localhost:4001/page"`
- `click "Button Text"`
- `enter "text" into "Field Name"`
- `wait until page contains "Expected Text"`
- `check that page does not contain "Error"`
- `save screenshot as "test-checkpoint"`

## Success Criteria
- [ ] All test tasks converted to testRigor format
- [ ] Tests organized in logical directory structure
- [ ] Each test is executable and independent
- [ ] Comprehensive error checking included
- [ ] Master test suite created for full execution

## Notes
- Use **natural language** for element identification
- Include **wait strategies** for dynamic content
- Add **error checking** after each significant action
- Create **reusable test components** where possible
"""

        with open(instruction_file, 'w', encoding='utf-8') as f:
            f.write(instructions)

    async def _launch_claude_code_terminal(self, title: str, instruction_file: str, working_directory: str) -> None:
        """Launch Claude Code in a new WSL terminal"""
        import subprocess
        import sys

        try:
            console.print(f"[cyan]Launching Claude Code terminal: {title}[/cyan]")
            console.print(f"[dim]Instructions: {instruction_file}[/dim]")
            console.print(f"[dim]Working Directory: {working_directory}[/dim]")

            # Convert Windows paths to WSL paths
            wsl_working_dir = working_directory.replace("D:\\", "/mnt/d/").replace("\\", "/")
            wsl_instruction_file = instruction_file.replace("D:\\", "/mnt/d/").replace("\\", "/")

            # Create the Claude Code command with correct syntax
            claude_command = f"cd /mnt/d/Repository/@Clients/FY.WB.Midway && claude -p '{wsl_instruction_file}'"

            if sys.platform == "win32":
                # Launch WSL terminal with Claude Code
                subprocess.Popen([
                    "wsl", "-d", "Ubuntu", "-e", "bash", "-c",
                    f"cd /mnt/d/Repository/@Clients/FY.WB.Midway && echo 'Starting {title}...' && echo 'Working Directory: {wsl_working_dir}' && {claude_command}"
                ], shell=False)

                console.print(f"[green]✅ Claude Code terminal launched for: {title}[/green]")
                console.print("[yellow]⚠️  Please complete the task in the Claude Code terminal before proceeding[/yellow]")
                console.print("[dim]The terminal will guide you through the process step by step[/dim]")

            else:
                # For non-Windows systems, use gnome-terminal or similar
                subprocess.Popen([
                    "gnome-terminal", "--title", title, "--",
                    "bash", "-c", claude_command
                ])

                console.print(f"[green]✅ Claude Code terminal launched for: {title}[/green]")

        except Exception as e:
            console.print(f"[red]Error launching Claude Code terminal: {e}[/red]")
            console.print("[yellow]You can manually run Claude Code with these commands:[/yellow]")
            console.print(f"[dim]wsl -d Ubuntu[/dim]")
            console.print(f"[dim]cd /mnt/d/Repository/@Clients/FY.WB.Midway[/dim]")
            console.print(f"[dim]claude -p '{wsl_instruction_file}'[/dim]")


# Example usage and testing functions
async def main():
    """Main function for testing the FrontendTestGenerator"""
    from pathlib import Path

    # Test configuration
    project_name = "FY.WB.Midway"
    base_path = Path("D:/Repository/@Clients/FY.WB.Midway")
    model_provider = "anthropic"

    # Initialize the generator
    generator = FrontendTestGenerator(project_name, base_path, model_provider)

    # Test the complete workflow
    console.print("[bold cyan]Testing Frontend Test Generator[/bold cyan]")

    result = await generator.run_complete_workflow()

    if result.success:
        console.print("[bold green]✅ Test generation completed successfully![/bold green]")
        console.print(f"Test Plan: {result.test_plan_path}")
        console.print(f"Tests Directory: {result.tests_directory}")
        console.print(f"Execution Time: {result.total_execution_time:.2f}s")
    else:
        console.print("[bold red]❌ Test generation failed![/bold red]")
        console.print(f"Error: {result.error_summary}")


if __name__ == "__main__":
    asyncio.run(main())
