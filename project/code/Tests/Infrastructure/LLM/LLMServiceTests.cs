using Xunit;
using FluentAssertions;
using Moq;
using Microsoft.Extensions.Logging;
using ByteForgeFrontend.Services.Infrastructure.LLM;
using ByteForgeFrontend.Services;
using Polly;

using System;
using System.Threading.Tasks;
namespace ByteForgeFrontend.Tests.Infrastructure.LLM;

public class LLMServiceTests
{
    private readonly Mock<ILLMProviderFactory> _mockProviderFactory;
    private readonly Mock<ILLMConfigurationService> _mockConfigService;
    private readonly Mock<ILogger<LLMService>> _mockLogger;
    private readonly LLMService _service;

    public LLMServiceTests()
    {
        _mockProviderFactory = new Mock<ILLMProviderFactory>();
        _mockConfigService = new Mock<ILLMConfigurationService>();
        _mockLogger = new Mock<ILogger<LLMService>>();
        
        var config = new LLMServicesConfiguration
        {
            MaxRetries = 3,
            TimeoutSeconds = 30,
            DefaultProvider = "openai"
        };
        
        _mockConfigService.Setup(x => x.Configuration).Returns(config);
        
        _service = new LLMService(_mockProviderFactory.Object, _mockConfigService.Object, _mockLogger.Object);
    }

    [Fact]
    public async Task GenerateAsync_WithSuccessfulProvider_ReturnsResponse()
    {
        // Arrange
        var mockProvider = new Mock<ILLMProvider>();
        var expectedResponse = new LLMGenerationResponse
        {
            Success = true,
            Content = "Generated content",
            Provider = "OpenAI",
            Model = "gpt-4o",
            TokensUsed = 100
        };
        
        mockProvider.Setup(x => x.GenerateAsync(It.IsAny<LLMGenerationRequest>(), It.IsAny<CancellationToken>()))
            .ReturnsAsync(expectedResponse);
        
        _mockProviderFactory.Setup(x => x.GetProvider("openai"))
            .Returns(mockProvider.Object);

        var request = new LLMGenerationRequest
        {
            Prompt = "Test prompt"
        };

        // Act
        var response = await _service.GenerateAsync(request);

        // Assert
        response.Should().NotBeNull();
        response.Success.Should().BeTrue();
        response.Content.Should().Be("Generated content");
        response.Provider.Should().Be("OpenAI");
    }

    [Fact]
    public async Task GenerateAsync_WithFailedProvider_TriesAlternativeProviders()
    {
        // Arrange
        var mockOpenAIProvider = new Mock<ILLMProvider>();
        var mockAnthropicProvider = new Mock<ILLMProvider>();
        
        var failedResponse = new LLMGenerationResponse
        {
            Success = false,
            Error = "API Error",
            Provider = "OpenAI"
        };
        
        var successResponse = new LLMGenerationResponse
        {
            Success = true,
            Content = "Generated by Anthropic",
            Provider = "Anthropic",
            Model = "claude-3",
            TokensUsed = 150
        };
        
        mockOpenAIProvider.Setup(x => x.GenerateAsync(It.IsAny<LLMGenerationRequest>(), It.IsAny<CancellationToken>()))
            .ReturnsAsync(failedResponse);
        
        mockAnthropicProvider.Setup(x => x.GenerateAsync(It.IsAny<LLMGenerationRequest>(), It.IsAny<CancellationToken>()))
            .ReturnsAsync(successResponse);
        
        _mockProviderFactory.Setup(x => x.GetProvider("openai"))
            .Returns(mockOpenAIProvider.Object);
        _mockProviderFactory.Setup(x => x.GetProvider("anthropic"))
            .Returns(mockAnthropicProvider.Object);
        _mockProviderFactory.Setup(x => x.GetAvailableProviders())
            .Returns(new[] { "openai", "anthropic" });

        var request = new LLMGenerationRequest
        {
            Prompt = "Test prompt"
        };

        // Act
        var response = await _service.GenerateAsync(request);

        // Assert
        response.Should().NotBeNull();
        response.Success.Should().BeTrue();
        response.Content.Should().Be("Generated by Anthropic");
        response.Provider.Should().Be("Anthropic");
    }

    [Fact]
    public async Task GenerateAsync_WithRetryableError_RetriesBeforeFailover()
    {
        // Arrange
        var mockProvider = new Mock<ILLMProvider>();
        var callCount = 0;
        
        mockProvider.Setup(x => x.GenerateAsync(It.IsAny<LLMGenerationRequest>(), It.IsAny<CancellationToken>()))
            .ReturnsAsync(() =>
            {
                callCount++;
                if (callCount < 3)
                {
                    throw new HttpRequestException("Temporary error");
                }
                return new LLMGenerationResponse
                {
                    Success = true,
                    Content = "Success after retry",
                    Provider = "OpenAI"
                };
            });
        
        _mockProviderFactory.Setup(x => x.GetProvider("openai"))
            .Returns(mockProvider.Object);

        var request = new LLMGenerationRequest
        {
            Prompt = "Test prompt"
        };

        // Act
        var response = await _service.GenerateAsync(request);

        // Assert
        response.Should().NotBeNull();
        response.Success.Should().BeTrue();
        response.Content.Should().Be("Success after retry");
        callCount.Should().Be(3);
    }

    [Fact]
    public async Task GenerateAsync_WithAllProvidersFailing_ReturnsError()
    {
        // Arrange
        var mockOpenAIProvider = new Mock<ILLMProvider>();
        var mockAnthropicProvider = new Mock<ILLMProvider>();
        
        var failedResponse = new LLMGenerationResponse
        {
            Success = false,
            Error = "API Error"
        };
        
        mockOpenAIProvider.Setup(x => x.GenerateAsync(It.IsAny<LLMGenerationRequest>(), It.IsAny<CancellationToken>()))
            .ReturnsAsync(failedResponse);
        
        mockAnthropicProvider.Setup(x => x.GenerateAsync(It.IsAny<LLMGenerationRequest>(), It.IsAny<CancellationToken>()))
            .ReturnsAsync(failedResponse);
        
        _mockProviderFactory.Setup(x => x.GetProvider("openai"))
            .Returns(mockOpenAIProvider.Object);
        _mockProviderFactory.Setup(x => x.GetProvider("anthropic"))
            .Returns(mockAnthropicProvider.Object);
        _mockProviderFactory.Setup(x => x.GetAvailableProviders())
            .Returns(new[] { "openai", "anthropic" });

        var request = new LLMGenerationRequest
        {
            Prompt = "Test prompt"
        };

        // Act
        var response = await _service.GenerateAsync(request);

        // Assert
        response.Should().NotBeNull();
        response.Success.Should().BeFalse();
        response.Error.Should().Contain("All LLM providers failed");
    }

    [Fact]
    public async Task GenerateAsync_WithSpecificProvider_UsesOnlyThatProvider()
    {
        // Arrange
        var mockProvider = new Mock<ILLMProvider>();
        var expectedResponse = new LLMGenerationResponse
        {
            Success = true,
            Content = "Generated by specific provider",
            Provider = "Anthropic"
        };
        
        mockProvider.Setup(x => x.GenerateAsync(It.IsAny<LLMGenerationRequest>(), It.IsAny<CancellationToken>()))
            .ReturnsAsync(expectedResponse);
        
        _mockProviderFactory.Setup(x => x.GetProvider("anthropic"))
            .Returns(mockProvider.Object);

        var request = new LLMGenerationRequest
        {
            Prompt = "Test prompt",
            PreferredProvider = "anthropic"
        };

        // Act
        var response = await _service.GenerateAsync(request);

        // Assert
        response.Should().NotBeNull();
        response.Success.Should().BeTrue();
        response.Provider.Should().Be("Anthropic");
        
        _mockProviderFactory.Verify(x => x.GetProvider("openai"), Times.Never);
    }

    [Fact]
    public async Task GenerateAsync_WithTimeout_CancelsRequest()
    {
        // Arrange
        var mockProvider = new Mock<ILLMProvider>();
        mockProvider.Setup(x => x.GenerateAsync(It.IsAny<LLMGenerationRequest>(), It.IsAny<CancellationToken>()))
            .Returns(async (LLMGenerationRequest req, CancellationToken ct) =>
            {
                await Task.Delay(TimeSpan.FromSeconds(60), ct);
                return new LLMGenerationResponse { Success = true };
            });
        
        _mockProviderFactory.Setup(x => x.GetProvider("openai"))
            .Returns(mockProvider.Object);

        var request = new LLMGenerationRequest
        {
            Prompt = "Test prompt"
        };

        // Act & Assert
        await Assert.ThrowsAsync<OperationCanceledException>(async () =>
        {
            using var cts = new CancellationTokenSource(TimeSpan.FromMilliseconds(100));
            await _service.GenerateAsync(request, cts.Token);
        });
    }
}